# offline_llm
Run your LLMs Locally with Ollama


 
## Locally Hosted LLM Chat  Works totally offline


This is a quick demo of how to get a fully local model up and running with Mistral 7b check out the accompanying [blog post](https://TKTKTK) for more details.



## Install dependencies
Ollama, an accessible tool for MacOS, Linux, and Windows users, offers seamless installation from its official website https://ollama.com/download. Upon completion, you will find a llama symbol

 just open your terminal & run:
```
$ ollama run mistral
```
```
pip install -r requirements.txt
```
A very simple streamlit app that demonstrates how to use LLM chat in a web app.

```
streamlit run app.py
```
